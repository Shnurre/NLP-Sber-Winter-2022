{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Морфология\n",
    "В этом ноутбуке описана подготовка данных для задачи POS-tagging. А также пара простых моделей на keras, решающих данную задачу. Оригинальная задача и ноутбук есть в контесте: https://www.kaggle.com/c/rupos2018/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Загрузка корпуса\n",
    "Здесь мы прочитаем корпуса из csv и разложим их по спискам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для совместимости со вторым питоном\n",
    "from __future__ import print_function\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "ad156ee50218601b689e546907d0ce3109e155bd"
   },
   "outputs": [],
   "source": [
    "# Имена файлов с данными.\n",
    "TRAIN_FILENAME = \"data/input/train.csv\"\n",
    "TEST_FILENAME = \"data/input/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "fdbea001cf6b2aa7b763b4836050dc4fdabb6457"
   },
   "outputs": [],
   "source": [
    "# Считывание файлов.\n",
    "from collections import namedtuple\n",
    "WordForm = namedtuple(\"WordForm\", \"word pos gram\")\n",
    "\n",
    "def get_sentences(filename, is_train):\n",
    "    sentences = []\n",
    "    with io.open(filename, \"r\", encoding='utf-8') as r:\n",
    "        # Пропускаем заголовок\n",
    "        next(r)\n",
    "        sentence = [] # будем заполнять список предложений\n",
    "        for line in r:\n",
    "            # предложения отделены по '\\n'\n",
    "            if len(line.strip()) == 0:\n",
    "                if len(sentence) == 0:\n",
    "                    continue\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "                continue\n",
    "            if is_train:\n",
    "                # Формат: индекс\\tномер_в_предложении\\tсловоформа\\tPOS#Грамемы\n",
    "                word = line.strip().split(\"\\t\")[2]\n",
    "                pos = line.strip().split(\"\\t\")[3].split(\"#\")[0]\n",
    "                gram = line.strip().split(\"\\t\")[3].split(\"#\")[1]\n",
    "                sentence.append(WordForm(word, pos, gram))\n",
    "            else:\n",
    "                word = line.strip().split(\"\\t\")[2]\n",
    "                sentence.append(word)\n",
    "        if len(sentence) != 0:\n",
    "            sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "fe4e1691bc5ef95abc711c5f403b27b897647878",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = get_sentences(TRAIN_FILENAME, True)\n",
    "test = get_sentences(TEST_FILENAME, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "А \t CONJ \t _\n",
      "ведь \t PART \t _\n",
      "для \t ADP \t _\n",
      "конкретных \t ADJ \t Case=Gen|Degree=Pos|Number=Plur\n",
      "изделий \t NOUN \t Animacy=Inan|Case=Gen|Gender=Neut|Number=Plur\n",
      "зачастую \t ADV \t Degree=Pos\n",
      "нужен \t ADJ \t Degree=Pos|Gender=Masc|Number=Sing|Variant=Brev\n",
      "монокристалл \t NOUN \t Animacy=Inan|Case=Nom|Gender=Masc|Number=Sing\n",
      "не \t PART \t _\n",
      "только \t PART \t _\n"
     ]
    }
   ],
   "source": [
    "# Выыедем, что получилось\n",
    "for wordform in train[0][:10]:\n",
    "    print(wordform.word, '\\t', wordform.pos, '\\t', wordform.gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для простоты далее будем использовать токены слов и POS-теги. Но чтобы определять грамматические значения нужно еще провести некоторые манипуляции с данными, описанные в оригинальном ноутубке. Мы же ограничимся только определением частей речи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Подготовка эмбеддингов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно в качестве признаков для обучения сеток используются словные эмбеддинги. Для этого можно скачать предобученные и сохранить их в матрицу, где в расположатся векторы эмбеддингах по индексам, соответсвующих слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#запомним все уникальные слова и POS-теги в корпусе\n",
    "word_set = set()\n",
    "pos_set = set()\n",
    "for sent in train:\n",
    "    for wordform in sent:\n",
    "        word_set.add(wordform.word.lower())\n",
    "        pos_set.add(wordform.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "подлежала, уральское, бюргерам, коваными, девятого, уральском, правильную, медном, страданием, изношенного, \n",
      "set([u'ADV', u'NOUN', u'ADP', u'PRON', u'SCONJ', u'PROPN', u'DET', u'SYM', u'INTJ', u'PUNCT', u'PART', u'X', u'AUX', u'NUM', u'CONJ', u'ADJ', u'VERB'])\n"
     ]
    }
   ],
   "source": [
    "for word in list(word_set)[:10]: \n",
    "    print(word, end=', ')\n",
    "print()\n",
    "print(pos_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Загрузите эмбеддинги c https://nlp.stanford.edu/projects/glove/ или другие, которые вам нравятся и пропишите путь к ним\n",
    "import numpy as np\n",
    "\n",
    "word_embeddings_path = 'data\\glove.6B.50d.txt'\n",
    "word2idx = {}\n",
    "word_embeddings = []\n",
    "embedding_size = None\n",
    "#Загружаем эмбеддинги\n",
    "with io.open(word_embeddings_path, 'r', encoding=\"utf-8\") as f_em:\n",
    "    for line in f_em:\n",
    "        split = line.strip().split(\" \")\n",
    "        # Совсем короткие строки пропускаем\n",
    "        if len(split) <= 2:\n",
    "            continue\n",
    "        # Встретив первую подходящую строку, фиксируем размер эмбеддингов\n",
    "        if embedding_size is None:\n",
    "            embedding_size = len(split) - 1\n",
    "            # Также нициализируем эмбеддинги для паддингов и неизвестных слов\n",
    "            word2idx[\"PADDING_TOKEN\"] = len(word2idx)\n",
    "            word_embeddings.append(np.zeros(embedding_size))\n",
    "\n",
    "            word2idx[\"UNKNOWN_TOKEN\"] = len(word2idx)\n",
    "            word_embeddings.append(np.random.uniform(-0.25, 0.25, embedding_size))\n",
    "        # После этого все эмбеддинги должны быть одинаковой длины\n",
    "        if len(split) - 1 != embedding_size:\n",
    "            continue\n",
    "            \n",
    "        #Если слова нет в корпусе, то не будем для него запоминать эмбеддинг        \n",
    "        if (split[0] not in word_set):\n",
    "            continue\n",
    "        \n",
    "        word_embeddings.append(np.asarray(split[1:], dtype='float32'))\n",
    "        word2idx[split[0]] = len(word2idx)\n",
    "\n",
    "word_embeddings = np.array(word_embeddings, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1947"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_set & set(word2idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98880"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как-то эмбеддинги не сильно подходят для данного корпуса поэтому, просто инициализируем рандмно матрицу эмбеддингов при определении сетки. Вам же предлагается все-таки поискать подходящие эмбеддинги и использовать их при обучении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. Подготовка данных\n",
    "Теперь нам остается только пронумеровать все слова и POS-теги и можно переходить к обучению сеток."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {'PAD' : 0, 'UNK' : 1}\n",
    "for word in word_set:\n",
    "    word_to_index[word] = len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_to_index = {}\n",
    "index_to_pos = {}\n",
    "for pos in pos_set:\n",
    "    pos_to_index[pos] = len(pos_to_index)\n",
    "    index_to_pos[len(index_to_pos)] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для полносвязной сетки просто захреначим все в один список\n",
    "data_X = []\n",
    "data_Y = []\n",
    "for sent in train:\n",
    "    for wordform in sent:\n",
    "        data_X.append(word_to_index[wordform.word.lower()])\n",
    "        data_Y.append(pos_to_index[wordform.pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43904, 87202, 73490, 80225, 49180, 73764, 31229, 5478, 65911, 66276]\n",
      "[14, 10, 2, 15, 1, 0, 15, 1, 10, 10]\n"
     ]
    }
   ],
   "source": [
    "print(data_X[:10])\n",
    "print(data_Y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 4. Полносвязная сеть\n",
    "Самой простой моделью является обычный перцептрон. На вход сетки будем подавать просто эмдеддинг каждого слова, на выходе ожидать распредедение вероятностей по тегам. В качестве фреймворка достаточно будет использовать keras и его Sequential модель (https://keras.io/models/sequential/), в которую слои добавляются последовательно, с помощью метода `add`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Activation, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 1, 50)             4944100   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 17)                1717      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 17)                0         \n",
      "=================================================================\n",
      "Total params: 4,950,917\n",
      "Trainable params: 6,817\n",
      "Non-trainable params: 4,944,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# на самом деле на вход сетки будет добавляться индекс слова, а слой эмбеддинга будет возвращать для него вектор\n",
    "model.add(Embedding(input_length=1, input_dim=len(word_to_index), output_dim=50, embeddings_initializer='random_uniform',\n",
    "                    trainable=False)) # матрицу эмбеддингов просто инициализируем нормальным распределением и отключим обучение\n",
    "# далее нам нужно схлопнуть трехмерный тензор с одной фиктивной размерностью в двумерный\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100)) # основной полносвязный слой\n",
    "model.add(Activation('relu')) # для приличия добавим функцию активации\n",
    "model.add(Dense(len(pos_to_index))) # выходной слой тоже полносвязный размерности по кол-ву тегов\n",
    "model.add(Activation('softmax')) # ну и в конце делаем softmax, чтобы получить распределение\n",
    "model.summary() # вывод получившейся модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# компилируем модель\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "850689/850689 [==============================] - 4953s 6ms/step - loss: 1.4284 - accuracy: 0.5380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x35cc3908>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# и обучаем\n",
    "model.fit(np.array(data_X), np.array(data_Y), epochs=1, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка обученности модели остается за вами. Этот пример лишь для того, чтобы показать как собрать сетку и скормить ей данные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 5. Рекуррентая сеть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее рассмотрим более приближенную к SOTA модель. Ей является рекуррентая сеть, которая принимает эмбеддинги слов в предложении и генерирует для них распределение вероятностей. Основным отличием от прошлой в том, что теперь мы будем использовать соседние слова как раз за счет рекуррентого слоя. Для этой модели мы уже будем использовать функциональный способ задания модели все того же кераса (https://keras.io/models/model/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, TimeDistributed,Bidirectional, Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 50)          4944100   \n",
      "_________________________________________________________________\n",
      "blstm (Bidirectional)        (None, None, 200)         120800    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 17)          3417      \n",
      "=================================================================\n",
      "Total params: 5,068,317\n",
      "Trainable params: 124,217\n",
      "Non-trainable params: 4,944,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# В начале задается входной слой, в котором мы укажем входную размерность. \n",
    "# Это будет None, т.к. мы заранее не знаем, какой будет длина каждого предложения \n",
    "input_layer = Input(shape=(None,), name='input')\n",
    "# Далее идет все тот же слой эмеддинга, которому мы на вход подаем предыдущий слой (в этом и суть functional APO)\n",
    "embeddings_layer = Embedding(input_dim=len(word_to_index), output_dim=50, \n",
    "                             trainable=False, embeddings_initializer='random_uniform',\n",
    "                             name='embedding')(input_layer)\n",
    "# Итак, основным слоем здесь будет двусторонний LSTM, который будет возвращать вектор для каждого слова (return_sequences=True) \n",
    "blstm_layer = Bidirectional(LSTM(100, return_sequences=True), name='blstm')(embeddings_layer)\n",
    "# Аналогично т.к. у нас здесь вектора для каждого слоя, то и полносвязный слой должен применяться для каждого слоя \n",
    "# по-отдельности. Поэтому полносвязный слой оборачивается в  TimeDistributed\n",
    "result_layer = TimeDistributed(Dense(len(pos_to_index), activation='softmax', name='result'))(blstm_layer)\n",
    "# собственно определяем модель входными и выходными слоями\n",
    "model = Model(inputs=[input_layer], outputs=result_layer)\n",
    "# компилируем\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# выводим архитектуру\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее нам нужно было бы распределить слова по предложениям, распределить по группам по длине, выравнить предложения по длине в одной групе, заполнив недостающие слова паддингами. Но это довольно неприятный процесс, а мне просто хочется запустить сетку и проверить, что она вообще работает, что сошлись все разверности. Поэтому просто раскидаем по 10 слов с помощью `numpy.reshape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnX = np.reshape(data_X[:850000], (-1,10))\n",
    "rnnY = np.reshape(data_Y[:850000], (-1,10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85000L, 10L, 1L)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(rnnX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну и проверим, что оно вообще работает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "85000/85000 [==============================] - 26455s 311ms/step - loss: 2.0810 - accuracy: 0.3562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x41795080>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(rnnX, rnnY, epochs=1, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 6. Задание\n",
    "В качестве упражения предлагается довести до ума обучения второй модели: распределить слова по предложениям, написать тестирование модели и собственно посмотреть как оно обучилось. Тестировать предлагаю на последней 1000 предложений, обучать - на остальном. Кто уверен в своих желаниях, то может решить оригинальную задачу: предсказывать также грамматические категории. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
